# Setup for 3scale 2.15 Performance Testing Using Locust

## Setup 3scale

Required logged in on the oc cli to an osd cluster 
Clone the following repo
```bash
git clone git@github.com:3scale-labs/3scale-perf-setup.git
cd 3scale-perf-setup
```

Run the scripts in the following order
```bash
./setup_db.sh
./setup_apimanager.sh
./setup_monitoring.sh
./setup_product.sh
```
>**NOTE:** You may need to `chmod 771 setup_*` before you can run the above scripts

## Setting up locust

Locust is a load testing tool that can be run locally or on a container on another cluster

* Fork repo - [https://github.com/integr8ly/locust-integreatly-operator](https://github.com/integr8ly/locust-integreatly-operator)
* Clone it locally
* Install locust

```bash
pip3 install locust 
```

**Config**

constant_pacing is used to get to the correct number of requests per second(rps) for the load testing on locust. It also takes into account the number of users. It is the inverse of constant_throughput. In order to increase the rps you need to decrease the constant pacing value. When changing this be sure to only update in small increments. More info on this function can be found [here](https://docs.locust.io/en/stable/api.html#locust.wait_time.constant_pacing).

| SKU | calculation | constant_pacing | users | RPS | 
| ---- | ----------- | --------------- | ----- | --- |
| 1 k | 100,000 ÷ 60 ÷ 60 ÷ 24 = 1.15740741 | 9.25 |11 | 1.157 |
| 1 million | 1 million ÷ 60 ÷ 60 ÷ 24 = 11.5740741 | 1.25 | 14 | 11.57 |
| 5 million | 5 million ÷ 60 ÷ 60 ÷ 24 = 57.8703704 | 0.207| 12 | 57.87 |
| 10 million | 10 million ÷ 60 ÷ 60 ÷ 24 = 115.740741 | 0.09 | 10 | 110 |
| 20 million | 20 million ÷ 60 ÷ 60 ÷ 24 = 231.481481 | 1 | 232 | 232 |
| 50 million | 50 million ÷ 60 ÷ 60 ÷ 24 = 578.703704 | 0.01 | 32 | 550 |
| 100 million | 100 million ÷ 60 ÷ 60 ÷ 24 = 1157.40741 | 0.05 | 59 | 1150 |


**NOTE:** we need to alter the constant_pacing value. We can set the constant_pacing variable to the value in the table above. This can be updated in locustfile.py

e.g.
```python
wait_time = constant_pacing(9.25) 
```

<br>

## Auth file
Locust requires an auth file. 

TOML is the recommended file format.

## TOML
```
[3scale]
url = "localhost:8000" # 3scale tenant URL
param = "?user_key=<key_here>"

```



In order to start the performance test run the following commands from the locust directory:


```bash
pipenv shell
locust
./start.sh
```


In your browser navigate to localhost:8089

To stop the test run


```bash
./kill.sh
```

<br>

## Running locust on a VM

### **EC2 Instance AWS**

Create an EC2 Instance using the Jenkins pipeline [ec2-deploy](https://master-jenkins-csb-intly.apps.ocp-c1.prod.psi.redhat.com/job/ManagedAPI/job/ec2-deploy/). You will need an ssh key.

Once this is created, ssh into the instance:


1. Install [AWS CLI](https://aws.amazon.com/cli/)
2. Ensure that the EC2 instance can take traffic from your IP address:

```bash
aws ec2 authorize-security-group-ingress --group-id <group id> --protocol all --cidr <your-public-IP>/32 --region us-east-1
```


3. SSH into the instance
    1. In your local terminal create a file pipelineKey.pem and open the file for editing
    2. Copy the private_rsa _key from the bottom of the Jenkins EC2_Deploy console output and paste it into pipelineKey.pem, save the file.
    3. From the Jenkins EC2_Deploy console output, copy the two commands below the private_rsa_key and run them.
    4. You should be in the EC2 instance :  example - [ec2-user@ip-10-11-128-177]$
4. Set up Locust
    1. Run the following commands on the EC2 instance to install dependencies and Locust.

```bash
ssh-keygen -t rsa -N "" -f .ssh/id_rsa
cat .ssh/id_rsa.pub >> .ssh/authorized_keys
ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa localhost 'exit'
sudo dnf install python3.11 -y
pip3.11 install locust
mkdir locust
```


2. Copy the required files from your local machine

```bash
scp -i /path/key-pair-name.pem locust/* ec2-user@\[instance-IPv6-address\]:locust
```


5. Add your public key to the authorized_keys of the ec2 instance
    1. First you will need to get your public key from your local machine.  Open another terminal and run the following command.

```bash
cat ~/.ssh/id_rsa.pub
```

2. Copy the output and return to the EC2 instance.  Paste the public key to the end of authorized_keys and save the file. On the EC2 instance authorized_keys can be found in ~/.ssh/authorized_keys

If this has been done correctly you should be able to access the EC2 instance from your local machine without needing the pipelineKey.pem file.

<br> 
Start the performance testing by running the start script


```bash
~/locust/start.sh
```

You can then navigate to the locust UI via http://&lt;remote.url>:8089

<br>

## Gathering Stats

We gather stats for our runs in this spreadsheet [https://docs.google.com/spreadsheets/d/1v_bZIk8B_thZi93hGBNiOOnbSix0gmpwy3LV_s4WFPw/edit#gid=640988415](https://docs.google.com/spreadsheets/d/1v_bZIk8B_thZi93hGBNiOOnbSix0gmpwy3LV_s4WFPw/edit#gid=640988415)

\
Row 18 to 49 are generated with this script [https://github.com/integr8ly/integreatly-operator/blob/master/scripts/capture_resource_metrics.sh](https://github.com/integr8ly/integreatly-operator/blob/master/scripts/capture_resource_metrics.sh)

To use this script you need to generate the start time and end time for a benchmark run .

**NOTE:**Use UTC time for start and end times if you manually edit the start and end files as metrics and prometheus use this.


```bash
# usually run before test
date -u +%Y-%m-%dT%TZ > perf-test-start-time.txt
# usually run after test
date -u +%Y-%m-%dT%TZ > perf-test-end-time.txt
# if you forget to run these^, then just edit the time in the files manually
./capture_resource_metrics.sh
# sample output
48
24
183.77841186523438
92.21733093261719
14.700000000000003
26185.745791999998
11191.551085069445
109.91796875
1664.9537760416663
1191.9760850694452
7119.827300347223
22152.960546875
0.0664819757528242
0.002929432877246077
0.031521544633803134
0.02088011667501379
0.15907593492959202
0.35843971739041935
11233.95703125
110.28125
1772.9296875
1195.2421875
7708.9375
22804.77734375
2.0267126406978004
0.1381565234045845
2.2181965388711644
0.02486586064388841
0.23639563371458033
4.740919111273934
```


We capture snapshots for some grafana dashboards in rows 51 to 58

redhat-rhoam-observability is used with the following dashboards


* Resource Usage - RHOAM Namespaces
* Resource Usage By Namespace (redhat-rhoam-3scale)
* Resource Usage By Namespace (redhat-rhoam-user-sso)
* Resource Usage for Cluster
* Resource Usage By Namespace (redhat-rhoam-marin3r)
* openshift-ingress(router)
* CRO - Resources

You can set the date and time in the top right corner - use UTC time.


<img src="images/grafana-set-date-and-time.png" width="65%" height="65%">


<br>
From the locust report you can get the 90th percentile. Before stopping the tests, download the test data here:

<img src="images/locust-download-data.png" width="40%" height="40%">

<br> 

<img src="images/locust-statistics.png" width="40%" height="40%">



Create token(login) and Get can be taken straight from the chart. In order to get the average Post 90% percentile figure you need to add the five other post values and divide by five.

The downloaded report can also be added to the relevant jira ticket. After you click download report, you must click download again.

<br>

## Alerts

We have a script that can be run while a load test is running to determine if any alerts are triggered during the load test. This is useful in overnight tests as you can't always be available to look at the dashboards. Its in the [script directory of the integreatly-operator](https://github.com/integr8ly/integreatly-operator/blob/master/scripts/alerts-during-perf-testing.sh)

```bash
# USAGE
# ./alerts-during-perf-testing.sh <optional product-name>
# ^C to break
# Generates two files one for firing and one for pending alerts
#
# PREREQUISITES
# - jq
# - oc (logged in at the cmd line in order to get the bearer token)
```


To view alerts manually, navigate to the redhat-rhoam-obserability namespace, choose Networking and then Routes from the menu on the left. Click on the Location URL for the Prometheus route. Log in with kubeadmin credentials. Then click on the Alerts tab to view their current state.

To check the history

* Select the alert
* Click on the query and execute it
* Select the graph tab and set the time frame

<img src="images/prometheus-alert-history.png" width="40%" height="40%">


